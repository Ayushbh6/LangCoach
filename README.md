# ğŸ—£ï¸ Language Coach MVP

> **Self-hosted AI language coach** that runs 30â€“45 minute speaking-first sessions, tracks errors, and shows measurable progress within 30 days.

[![TypeScript](https://img.shields.io/badge/TypeScript-007ACC?style=flat&logo=typescript&logoColor=white)](https://www.typescriptlang.org/)
[![Next.js](https://img.shields.io/badge/Next.js-000000?style=flat&logo=next.js&logoColor=white)](https://nextjs.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](./LICENSE)

---

## âœ¨ Features

- **Bring Your Own Key (BYOK)**: Full provider abstraction for LLM, Speech-to-Text, and Text-to-Speech
- **Multi-Provider Support**: OpenAI, Google Gemini, OpenRouter, Ollama, and more
- **Modern Stack**: Next.js 16+ (App Router), TypeScript, Tailwind CSS, shadcn/ui
- **Docker Ready**: Containerized development environment with volume persistence
- **Latest AI SDKs**: OpenAI Responses API, Google GenAI, OpenRouter SDK

---

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
npm install
```

### 2. Configure Environment

Create a `.env` file in the project root:

```bash
# Provider Selection
LLM_PROVIDER=openai          # openai | gemini | openrouter | ollama
STT_PROVIDER=openai          # openai | gemini
TTS_PROVIDER=openai          # openai | gemini | webspeech

# API Keys (add only what you need)
OPENAI_API_KEY=sk-...
GEMINI_API_KEY=...
OPENROUTER_API_KEY=...
ANTHROPIC_API_KEY=...

# Local Models (optional)
OLLAMA_BASE_URL=http://localhost:11434
LMSTUDIO_BASE_URL=http://localhost:1234
```

### 3. Run Development Server

```bash
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) to view the app.

---

## ğŸ“¦ Project Structure

```
lang-coach/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/                    # Next.js App Router
â”‚   â”‚   â”œâ”€â”€ api/               # API routes (health, config)
â”‚   â”‚   â”œâ”€â”€ settings/          # Settings page
â”‚   â”‚   â””â”€â”€ page.tsx           # Home page
â”‚   â”œâ”€â”€ components/            # React components
â”‚   â”‚   â”œâ”€â”€ app-shell.tsx      # Main layout
â”‚   â”‚   â”œâ”€â”€ theme-provider.tsx
â”‚   â”‚   â””â”€â”€ theme-toggle.tsx
â”‚   â””â”€â”€ lib/
â”‚       â”œâ”€â”€ config/            # Environment & provider config
â”‚       â””â”€â”€ providers/         # Provider abstraction layer
â”‚           â”œâ”€â”€ llm/           # OpenAI, Gemini, OpenRouter, Ollama
â”‚           â”œâ”€â”€ transcription/ # Speech-to-Text providers
â”‚           â”œâ”€â”€ tts/           # Text-to-Speech providers
â”‚           â”œâ”€â”€ storage/       # SQLite (in progress)
â”‚           â”œâ”€â”€ types.ts       # Core interfaces
â”‚           â””â”€â”€ index.ts       # Factory functions
â”œâ”€â”€ docker/                    # Docker configuration
â”œâ”€â”€ docs/                      # Documentation
â””â”€â”€ plan/                      # Project planning docs
```

---

## ğŸ› ï¸ Technology Stack

| Category | Technology |
|----------|-----------|
| **Framework** | Next.js 16+ (App Router) |
| **Language** | TypeScript 5+ |
| **Styling** | Tailwind CSS 4, shadcn/ui |
| **AI SDKs** | OpenAI v6+, @google/genai, @openrouter/sdk |
| **Database** | SQLite (planned: Drizzle ORM) |
| **Deployment** | Docker, Docker Compose |

---

## ğŸ¯ Current Status

### âœ… Phase 0: Foundation (Complete)
- [x] Next.js + TypeScript + Tailwind setup
- [x] Docker environment with volume mounts
- [x] BYOK configuration system
- [x] Settings UI with dark mode
- [x] Health & config API endpoints

### ğŸ”„ Phase 1: Provider Abstraction (In Progress)
- [x] Core provider interfaces (`LLMProvider`, `TranscriptionProvider`, `TTSProvider`, `StorageProvider`)
- [x] OpenAI integration (Responses API, `gpt-4o-transcribe`, `gpt-4o-mini-tts`)
- [x] Google Gemini integration (`gemini-3-flash-preview`, `gemini-2.5-flash-preview-tts`)
- [x] OpenRouter SDK integration
- [x] Ollama support
- [ ] SQLite/ORM setup (T06 - Next)
- [ ] Database schemas (T07)

---

## ğŸ”§ Provider Configuration

The app uses a **factory pattern** to switch between providers at runtime based on environment variables.

### Supported Providers

#### LLM (Language Models)
- **OpenAI**: `gpt-5.1-chat-latest` (Responses API)
- **Google Gemini**: `gemini-3-flash-preview`
- **OpenRouter**: `minimax/minimax-m2-her` (or any OpenRouter model)
- **Ollama**: `llama3.1` (local)

#### Speech-to-Text (STT)
- **OpenAI**: `gpt-4o-transcribe`
- **Gemini**: (planned)

#### Text-to-Speech (TTS)
- **OpenAI**: `gpt-4o-mini-tts` (voice: `coral`)
- **Gemini**: `gemini-2.5-flash-preview-tts` (voice: `Kore`)
- **Web Speech API**: Browser-based (client-side only)

---

## ğŸ³ Docker Development

```bash
# Build and run containers
docker-compose up --build

# Run in detached mode
docker-compose up -d

# View logs
docker-compose logs -f

# Stop containers
docker-compose down
```

---

## ğŸ§ª API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/health` | GET | Health check for all configured providers |
| `/api/config` | GET | Returns current provider configuration |

---

## ğŸ¤ Contributing

This is an early-stage MVP. Contributions, issues, and feature requests are welcome!

---

## ğŸ“„ License

[MIT](./LICENSE)

---

## ğŸ—ºï¸ Roadmap

- **Phase 2**: Core conversation runner & session management
- **Phase 3**: Error tracking & correction flow
- **Phase 4**: Language Packs (German, French, Spanish)
- **Phase 5**: Progress tracking & analytics
